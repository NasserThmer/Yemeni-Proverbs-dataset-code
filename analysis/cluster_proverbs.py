# -*- coding: utf-8 -*-
"""cluster_proverbs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-IKCZACdke_Dbw2sOTrRMv8T4eXpt1d
"""

import os
import requests
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import umap
import hdbscan
import matplotlib.pyplot as plt
from collections import Counter
import re

# --------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø± ÙˆØ§Ù„Ø±Ø§Ø¨Ø·
# --------------------------
DATA_PATH = "data/Yemeni_proverbs.json"
DATA_URL = "https://zenodo.org/record/XXXXXXX/files/Yemeni_proverbs.json?download=1"  # â† Ø§Ø³ØªØ¨Ø¯Ù„Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ

# --------------------------
# ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
# --------------------------
if not os.path.exists(DATA_PATH):
    os.makedirs("data", exist_ok=True)
    print(f"â¬‡ï¸ Downloading dataset from {DATA_URL} ...")
    response = requests.get(DATA_URL)
    if response.status_code == 200:
        with open(DATA_PATH, "wb") as f:
            f.write(response.content)
        print("âœ… Dataset downloaded successfully.")
    else:
        raise Exception(f"âŒ Failed to download dataset. Please check the URL or your internet connection.")

# --------------------------
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# --------------------------
df = pd.read_json(DATA_PATH)
texts = df["proverb"].astype(str).tolist()

# --------------------------
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# --------------------------
print("ğŸ”„ Loading Arabic-Triplet-Matryoshka-V2 model...")
model = SentenceTransformer("Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2")

print("ğŸ”„ Generating embeddings...")
embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)

# --------------------------
# ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ (UMAP)
# --------------------------
umap_reducer = umap.UMAP(n_components=2, metric='cosine', random_state=42)
X_umap = umap_reducer.fit_transform(embeddings)

# --------------------------
# Ø§Ù„ØªØ¬Ù…ÙŠØ¹ (HDBSCAN)
# --------------------------
clusterer = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean', prediction_data=True)
labels = clusterer.fit_predict(X_umap)
df["cluster"] = labels

# --------------------------
# Ø§Ù„ØªØµÙˆÙŠØ±
# --------------------------
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, cmap='tab20', s=20)
plt.title("Yemeni Proverbs Thematic Clustering\n(Arabic-Triplet-Matryoshka-V2 + UMAP + HDBSCAN)")
plt.xlabel("UMAP Component 1")
plt.ylabel("UMAP Component 2")
plt.colorbar(scatter, label="Cluster ID")
plt.grid(True)
plt.tight_layout()
plt.show()

# --------------------------
# Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©
# --------------------------
def tokenize(text):
    text = re.sub(r"[^\w\s]", "", text)
    return text.split()

print("\nğŸ“‹ Top Keywords by Cluster:\n")
for cluster_id in sorted(df["cluster"].unique()):
    if cluster_id == -1:
        continue
    cluster_texts = df[df["cluster"] == cluster_id]["proverb"].tolist()
    tokens = []
    for text in cluster_texts:
        tokens.extend(tokenize(text))
    top_words = [word for word, freq in Counter(tokens).most_common(10)]
    print(f"ğŸŸ© Cluster {cluster_id} â€” Total Proverbs: {len(cluster_texts)}")
    print("Top Keywords:", top_words)
    print("-" * 50)