# -*- coding: utf-8 -*-
"""train_arat5v2.py.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-IKCZACdke_Dbw2sOTrRMv8T4eXpt1d
"""

import json
import re
import argparse
from datasets import Dataset, DatasetDict, load_metric
from transformers import (
    T5Tokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)

# ------------------------------------------
# Normalization and cleaning functions
# ------------------------------------------

def remove_diacritics(text):
    return re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)

def normalize_arabic(text):
    text = re.sub("[إأآا]", "ا", text)
    text = text.replace("ة", "ه")
    text = text.replace("ى", "ي")
    text = text.replace("ؤ", "و").replace("ئ", "ي")
    return text

def preprocess(example):
    proverb = normalize_arabic(remove_diacritics(example["proverb"]))
    explanation = normalize_arabic(remove_diacritics(example["explanation"]))
    return {
        "input": f"اشرح المثل اليمني: {proverb}",
        "target": explanation
    }

def tokenize(example, tokenizer):
    inputs = tokenizer(example["input"], max_length=512, truncation=True, padding="max_length")
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["target"], max_length=128, truncation=True, padding="max_length")["input_ids"]
    labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]
    inputs["labels"] = labels
    return inputs

# ------------------------------------------
# Main training function
# ------------------------------------------

def main(train_file, val_file, model_name, output_dir):
    # Load data
    with open(train_file, "r", encoding="utf-8") as f:
        train_data = json.load(f)
    with open(val_file, "r", encoding="utf-8") as f:
        val_data = json.load(f)

    # Create DatasetDict
    dataset = DatasetDict({
        "train": Dataset.from_list(train_data),
        "validation": Dataset.from_list(val_data)
    })

    # Normalize and preprocess
    dataset = dataset.map(preprocess)

    # Load tokenizer and model
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    # Tokenize
    tokenized = dataset.map(lambda x: tokenize(x, tokenizer), batched=False)

    # Training arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=5e-5,
        warmup_ratio=0.1,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        weight_decay=0.01,
        num_train_epochs=20,
        predict_with_generate=True,
        generation_max_length=128,
        logging_dir="./logs",
        logging_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        lr_scheduler_type="linear",
        fp16=True,
        report_to="none",
        run_name="arat5v2-l4-5e5-linear"
    )

    # Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized["train"],
        eval_dataset=tokenized["validation"],
        tokenizer=tokenizer,
        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )

    # Train!
    trainer.train()


# ------------------------------------------
# CLI Entry point
# ------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Fine-tune AraT5v2 on Yemeni proverbs dataset.")
    parser.add_argument("--train_file", type=str, required=True, help="Path to training JSON file.")
    parser.add_argument("--val_file", type=str, required=True, help="Path to validation JSON file.")
    parser.add_argument("--model_name", type=str, default="UBC-NLP/AraT5v2-base-1024", help="Model name or path.")
    parser.add_argument("--output_dir", type=str, default="./arat5v2-finetuned", help="Directory to save the model.")

    args = parser.parse_args()
    main(args.train_file, args.val_file, args.model_name, args.output_dir)