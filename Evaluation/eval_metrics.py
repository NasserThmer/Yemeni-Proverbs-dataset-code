# -*- coding: utf-8 -*-
"""eval_metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-IKCZACdke_Dbw2sOTrRMv8T4eXpt1d
"""

import json
import argparse
import pandas as pd
import numpy as np
from tqdm import tqdm
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge import Rouge
from bert_score import score as bertscore
from sacrebleu.metrics import CHRF
from sentence_transformers import SentenceTransformer, util

# ------------------------------------------
# Evaluation function
# ------------------------------------------

def evaluate_metrics(json_path, save=True):
    print(f"üìÇ Loading file: {json_path}")
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    df = pd.DataFrame(data)

    # Load models
    print("üî§ Loading sentence-transformer models...")
    models = {
        "MiniLM": SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2"),
        "CAMeL": SentenceTransformer("CAMeL-Lab/bert-base-arabic-camelbert-ca"),
        "Arabic-KW": SentenceTransformer("medmediani/Arabic-KW-Mdel"),
        "SAS": SentenceTransformer("akhooli/Arabic-SBERT-100K")
    }

    # Initialize metrics
    bleu_scores = []
    rouge_scores = {"rouge-1": [], "rouge-2": [], "rouge-l": []}
    cosine_scores = {"MiniLM": [], "CAMeL": [], "Arabic-KW": []}
    sas_scores = []
    bertscore_f1 = []
    chrf_scores = []

    smooth = SmoothingFunction().method4
    rouge = Rouge()
    chrf = CHRF(word_order=2)

    print("‚öôÔ∏è Computing metrics...")
    for ref, pred in tqdm(zip(df["reference_explanation"], df["generated_explanation"]), total=len(df)):
        # BLEU
        bleu = sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth)
        bleu_scores.append(bleu)

        # ROUGE
        rouge_result = rouge.get_scores(pred, ref)[0]
        for k in rouge_scores:
            rouge_scores[k].append(rouge_result[k]["f"])

        # Cosine Similarity
        for name in ["MiniLM", "CAMeL", "Arabic-KW"]:
            emb1 = models[name].encode(ref, convert_to_tensor=True)
            emb2 = models[name].encode(pred, convert_to_tensor=True)
            cosine_scores[name].append(float(util.cos_sim(emb1, emb2)))

        # SAS
        emb1 = models["SAS"].encode(ref, convert_to_tensor=True)
        emb2 = models["SAS"].encode(pred, convert_to_tensor=True)
        sas_scores.append(float(util.cos_sim(emb1, emb2)))

        # BERTScore
        _, _, F1 = bertscore([pred], [ref], lang="ar", verbose=False)
        bertscore_f1.append(float(F1[0]))

        # chrF++
        chrf_score = chrf.sentence_score(pred, [ref]).score
        chrf_scores.append(chrf_score)

    # Add to dataframe
    df["BLEU"] = bleu_scores
    df["ROUGE-1"] = rouge_scores["rouge-1"]
    df["ROUGE-2"] = rouge_scores["rouge-2"]
    df["ROUGE-L"] = rouge_scores["rouge-l"]
    df["Cosine_MiniLM"] = cosine_scores["MiniLM"]
    df["Cosine_CAMeL"] = cosine_scores["CAMeL"]
    df["Cosine_Arabic_KW"] = cosine_scores["Arabic-KW"]
    df["SAS_SBERT_100K"] = sas_scores
    df["BERTScore_F1"] = bertscore_f1
    df["chrF++"] = chrf_scores

    # Save results
    if save:
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(df.to_dict(orient="records"), f, ensure_ascii=False, indent=2)
        print(f"‚úÖ Scores saved to: {json_path}")

    # Print averages
    metrics = [
        "BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L",
        "Cosine_MiniLM", "Cosine_CAMeL", "Cosine_Arabic_KW",
        "SAS_SBERT_100K", "chrF++", "BERTScore_F1"
    ]
    print("\nüìä Average Scores:")
    print(df[metrics].mean().round(4))


# ------------------------------------------
# Entry Point
# ------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate model outputs using BLEU, ROUGE, Cosine, SAS, BERTScore, and chrF++")
    parser.add_argument("--file", type=str, required=True, help="Path to JSON file with reference and generated explanations")

    args = parser.parse_args()
    evaluate_metrics(args.file)