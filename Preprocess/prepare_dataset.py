# -*- coding: utf-8 -*-
"""preprocess/prepare_dataset.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-IKCZACdke_Dbw2sOTrRMv8T4eXpt1d
"""

import re
import argparse
from datasets import load_dataset
from transformers import AutoTokenizer

# -------------------------
# Text normalization utilities
# -------------------------

def remove_diacritics(text):
    """
    Remove Arabic diacritics (tashkeel) from the text.
    """
    return re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)

def normalize_arabic(text):
    """
    Normalize Arabic letters to reduce spelling variation:
    - Replace different forms of Alef (ÿ£ÿå ÿ¢ÿå ÿ•) with simple Alef (ÿß)
    - Replace Teh Marbuta (ÿ©) with Heh (Ÿá)
    - Replace Alif Maqsura (Ÿâ) with Yeh (Ÿä)
    - Replace Hamza forms (ÿ§, ÿ¶) with Waw and Yeh
    """
    text = re.sub("[ÿ•ÿ£ÿ¢ÿß]", "ÿß", text)
    text = text.replace("ÿ©", "Ÿá")
    text = text.replace("Ÿâ", "Ÿä")
    text = text.replace("ÿ§", "Ÿà").replace("ÿ¶", "Ÿä")
    return text

def preprocess(example):
    """
    Apply normalization and diacritic removal to proverb and explanation.
    Format the input as a prompt for the model.
    """
    proverb = normalize_arabic(remove_diacritics(example["proverb"]))
    explanation = normalize_arabic(remove_diacritics(example["explanation"]))
    return {
        "input": f"ÿßÿ¥ÿ±ÿ≠ ÿßŸÑŸÖÿ´ŸÑ ÿßŸÑŸäŸÖŸÜŸä: {proverb}",  # Arabic prompt (do not change)
        "target": explanation
    }

def tokenize_function(example, tokenizer):
    """
    Tokenize the input and target using the given tokenizer.
    Convert padding tokens in labels to -100 for loss masking.
    """
    inputs = tokenizer(example["input"], max_length=512, truncation=True, padding="max_length")

    # Tokenize the target/explanation (using target tokenizer context)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["target"], max_length=128, truncation=True, padding="max_length")["input_ids"]

    # Replace padding tokens with -100 to ignore them in the loss
    labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]
    inputs["labels"] = labels
    return inputs

# -------------------------
# Main script
# -------------------------

def main(dataset_path, tokenizer_name, output_path):
    print("üì¶ Loading dataset...")
    dataset_dict = load_dataset("json", data_files={"train": dataset_path})

    # Apply normalization and prompt formatting
    dataset_dict = dataset_dict.map(preprocess)

    print("üî§ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    print("‚öôÔ∏è Tokenizing...")
    tokenized_dataset = dataset_dict.map(lambda ex: tokenize_function(ex, tokenizer), batched=False)

    print("üíæ Saving tokenized dataset...")
    tokenized_dataset["train"].to_json(output_path, force_ascii=False)
    print(f"‚úÖ Saved to: {output_path}")

# -------------------------
# Entry point
# -------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Normalize and tokenize the Yemeni Proverbs dataset.")
    parser.add_argument("--dataset_path", type=str, required=True, help="Path to the raw JSON file.")
    parser.add_argument("--tokenizer_name", type=str, default="UBC-NLP/AraT5v2-base-1024", help="Tokenizer model name or path.")
    parser.add_argument("--output_path", type=str, required=True, help="Path to save the processed JSON file.")

    args = parser.parse_args()
    main(args.dataset_path, args.tokenizer_name, args.output_path)