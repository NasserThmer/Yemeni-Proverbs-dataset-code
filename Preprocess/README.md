# ğŸ§¹ Preprocess Yemeni Proverbs Dataset

This script prepares a JSON dataset of Yemeni proverbs and explanations for training by:
- Removing Arabic diacritics
- Normalizing letter variations
- Formatting the input as a prompt
- Tokenizing the data using an Arabic seq2seq tokenizer

---

## ğŸ“ File Description

- `prepare_dataset.py`: Preprocess and tokenize script
- Input: Raw JSON file with fields:
  - `"proverb"`: Raw proverb string
  - `"explanation"`: Raw explanation
- Output: JSON file with fields:
  - `"input"`: Prompted and tokenized text
  - `"labels"`: Tokenized explanation (with padding masked)

---

## ğŸ§  Normalization Applied

- Remove tashkeel (diacritics)
- Normalize:
  - `Ø£`, `Ø¢`, `Ø¥` â†’ `Ø§`
  - `Ø©` â†’ `Ù‡`
  - `Ù‰` â†’ `ÙŠ`
  - `Ø¤`, `Ø¦` â†’ `Ùˆ`, `ÙŠ`

---

## ğŸš€ How to Run

### 1. Install dependencies

```bash
pip install transformers datasets
```

### 2. Run the script

```bash
python prepare_dataset.py \
    --dataset_path ./data/raw_proverbs.json \
    --tokenizer_name UBC-NLP/AraT5v2-base-1024 \
    --output_path ./data/tokenized_proverbs.json
```

---

## ğŸ“ Notes

- The output is suitable for training with Hugging Face `Trainer`.
- Tokenization replaces padding tokens in labels with `-100` to exclude them from loss computation.

---

## ğŸ“„ License

MIT License. See `LICENSE` file for terms.
